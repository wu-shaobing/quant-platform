# åç«¯é…ç½®æ–‡ä»¶è¯¦ç»†è¯´æ˜

## ğŸ”§ é…ç½®æ–‡ä»¶æ¶æ„

```mermaid
graph TD
    A[ç¯å¢ƒå˜é‡æ–‡ä»¶] --> B[åº”ç”¨é…ç½®]
    C[æ•°æ®åº“é…ç½®] --> B
    D[Redisé…ç½®] --> B
    E[CTPé…ç½®] --> B
    F[Celeryé…ç½®] --> B
    G[æ—¥å¿—é…ç½®] --> B
    H[å®‰å…¨é…ç½®] --> B
    
    B --> I[å¼€å‘ç¯å¢ƒ]
    B --> J[æµ‹è¯•ç¯å¢ƒ]
    B --> K[ç”Ÿäº§ç¯å¢ƒ]
```

## ğŸ“‹ ç¯å¢ƒå˜é‡é…ç½®

### .env.example (ç¯å¢ƒå˜é‡æ¨¡æ¿)
```bash
# =============================================================================
# é‡åŒ–äº¤æ˜“åç«¯ç¯å¢ƒé…ç½®æ¨¡æ¿
# =============================================================================

# åŸºç¡€é…ç½®
ENV=development
DEBUG=true
APP_NAME="Quant Trading Backend"
VERSION=1.0.0
SECRET_KEY=your-super-secret-key-change-this-in-production

# æœåŠ¡å™¨é…ç½®
HOST=0.0.0.0
PORT=8000
WORKERS=4
RELOAD=true

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://quant_user:quant_password@localhost:5432/quant_db
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20
DATABASE_ECHO=false

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=
REDIS_DB=0
REDIS_MAX_CONNECTIONS=100
REDIS_EXPIRE_TIME=3600

# CTPé…ç½®
CTP_BROKER_ID=9999
CTP_USER_ID=your_user_id
CTP_PASSWORD=your_password
CTP_MD_ADDRESS=tcp://180.168.146.187:10010
CTP_TD_ADDRESS=tcp://180.168.146.187:10000
CTP_APP_ID=simnow_client_test
CTP_AUTH_CODE=0000000000000000

# JWTé…ç½®
JWT_SECRET_KEY=your-jwt-secret-key
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# Celeryé…ç½®
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2
CELERY_TASK_SERIALIZER=json
CELERY_RESULT_SERIALIZER=json

# é‚®ä»¶é…ç½®
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-email-password
SMTP_TLS=true
SMTP_SSL=false

# ç¬¬ä¸‰æ–¹æœåŠ¡é…ç½®
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
PROMETHEUS_PORT=9090

# æ–‡ä»¶å­˜å‚¨é…ç½®
UPLOAD_PATH=./data/uploads
REPORTS_PATH=./data/reports
LOGS_PATH=./data/logs
MAX_FILE_SIZE=10485760  # 10MB

# å®‰å…¨é…ç½® (ä¿®å¤ï¼šæ·»åŠ Viteé»˜è®¤ç«¯å£5173)
CORS_ORIGINS=["http://localhost:5173", "http://localhost:3000", "http://localhost:8080"]
ALLOWED_HOSTS=["localhost", "127.0.0.1"]
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=60

# ç›‘æ§é…ç½®
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO
```

### .env.development (å¼€å‘ç¯å¢ƒ)
```bash
# å¼€å‘ç¯å¢ƒé…ç½®
ENV=development
DEBUG=true
RELOAD=true

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://dev_user:dev_pass@localhost:5432/quant_dev
DATABASE_ECHO=true  # å¼€å‘ç¯å¢ƒæ˜¾ç¤ºSQL

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0

# CTPé…ç½® (SimNowä»¿çœŸç¯å¢ƒ)
CTP_BROKER_ID=9999
CTP_MD_ADDRESS=tcp://180.168.146.187:10010
CTP_TD_ADDRESS=tcp://180.168.146.187:10000

# æ—¥å¿—é…ç½®
LOG_LEVEL=DEBUG

# å®‰å…¨é…ç½® (å¼€å‘ç¯å¢ƒå®½æ¾)
CORS_ORIGINS=["*"]
RATE_LIMIT_REQUESTS=1000
```

### .env.production (ç”Ÿäº§ç¯å¢ƒ)
```bash
# ç”Ÿäº§ç¯å¢ƒé…ç½®
ENV=production
DEBUG=false
RELOAD=false

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://prod_user:secure_password@prod-db:5432/quant_prod
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=0

# Redisé…ç½®
REDIS_URL=redis://:redis_password@prod-redis:6379/0
REDIS_PASSWORD=secure_redis_password

# CTPé…ç½® (ç”Ÿäº§ç¯å¢ƒ)
CTP_BROKER_ID=your_production_broker_id
CTP_USER_ID=your_production_user_id
CTP_PASSWORD=your_production_password
CTP_MD_ADDRESS=tcp://production-md-server:10010
CTP_TD_ADDRESS=tcp://production-td-server:10000

# å®‰å…¨é…ç½®
SECRET_KEY=your-super-secure-secret-key-for-production
JWT_SECRET_KEY=your-super-secure-jwt-key-for-production
CORS_ORIGINS=["https://yourdomain.com"]
ALLOWED_HOSTS=["yourdomain.com", "api.yourdomain.com"]

# ç›‘æ§é…ç½®
SENTRY_DSN=https://your-production-sentry-dsn@sentry.io/project-id
LOG_LEVEL=INFO
```

## âš™ï¸ åº”ç”¨é…ç½®ç±»

### app/core/config.py
```python
from pydantic import BaseSettings, validator
from typing import List, Optional, Union
import secrets
from functools import lru_cache

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®ç±»"""
    
    # åŸºç¡€é…ç½®
    APP_NAME: str = "Quant Trading Backend"
    VERSION: str = "1.0.0"
    ENV: str = "development"
    DEBUG: bool = False
    SECRET_KEY: str = secrets.token_urlsafe(32)
    
    # æœåŠ¡å™¨é…ç½®
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    WORKERS: int = 4
    RELOAD: bool = False
    
    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 10
    DATABASE_MAX_OVERFLOW: int = 20
    DATABASE_ECHO: bool = False
    
    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str]) -> str:
        if not v:
            raise ValueError("DATABASE_URL is required")
        return v
    
    # Redisé…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"
    REDIS_PASSWORD: Optional[str] = None
    REDIS_DB: int = 0
    REDIS_MAX_CONNECTIONS: int = 100
    REDIS_EXPIRE_TIME: int = 3600
    
    # CTPé…ç½®
    CTP_BROKER_ID: str
    CTP_USER_ID: str
    CTP_PASSWORD: str
    CTP_MD_ADDRESS: str
    CTP_TD_ADDRESS: str
    CTP_APP_ID: str = "simnow_client_test"
    CTP_AUTH_CODE: str = "0000000000000000"
    
    @validator("CTP_BROKER_ID", "CTP_USER_ID", "CTP_PASSWORD", pre=True)
    def validate_ctp_config(cls, v: str) -> str:
        if not v:
            raise ValueError("CTP configuration is required")
        return v
    
    # JWTé…ç½®
    JWT_SECRET_KEY: str = secrets.token_urlsafe(32)
    JWT_ALGORITHM: str = "HS256"
    JWT_ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    JWT_REFRESH_TOKEN_EXPIRE_DAYS: int = 7
    
    # Celeryé…ç½®
    CELERY_BROKER_URL: str = "redis://localhost:6379/1"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/2"
    CELERY_TASK_SERIALIZER: str = "json"
    CELERY_RESULT_SERIALIZER: str = "json"
    
    # é‚®ä»¶é…ç½®
    SMTP_HOST: Optional[str] = None
    SMTP_PORT: int = 587
    SMTP_USER: Optional[str] = None
    SMTP_PASSWORD: Optional[str] = None
    SMTP_TLS: bool = True
    SMTP_SSL: bool = False
    
    # ç¬¬ä¸‰æ–¹æœåŠ¡é…ç½®
    SENTRY_DSN: Optional[str] = None
    PROMETHEUS_PORT: int = 9090
    
    # æ–‡ä»¶å­˜å‚¨é…ç½®
    UPLOAD_PATH: str = "./data/uploads"
    REPORTS_PATH: str = "./data/reports"
    LOGS_PATH: str = "./data/logs"
    MAX_FILE_SIZE: int = 10 * 1024 * 1024  # 10MB
    
    # å®‰å…¨é…ç½®
    CORS_ORIGINS: List[str] = ["http://localhost:3000"]
    ALLOWED_HOSTS: List[str] = ["localhost", "127.0.0.1"]
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_WINDOW: int = 60
    
    @validator("CORS_ORIGINS", pre=True)
    def assemble_cors_origins(cls, v: Union[str, List[str]]) -> Union[List[str], str]:
        if isinstance(v, str) and not v.startswith("["):
            return [i.strip() for i in v.split(",")]
        elif isinstance(v, (list, str)):
            return v
        raise ValueError(v)
    
    # ç›‘æ§é…ç½®
    ENABLE_METRICS: bool = True
    METRICS_PORT: int = 9090
    LOG_LEVEL: str = "INFO"
    
    # ä¸šåŠ¡é…ç½®
    DEFAULT_PAGE_SIZE: int = 20
    MAX_PAGE_SIZE: int = 100
    DEFAULT_KLINE_LIMIT: int = 500
    MAX_KLINE_LIMIT: int = 5000
    
    # é£æ§é…ç½®
    MAX_ORDER_AMOUNT: float = 1000000.0  # å•ç¬”è®¢å•æœ€å¤§é‡‘é¢
    MAX_DAILY_LOSS: float = 50000.0      # å•æ—¥æœ€å¤§äºæŸ
    MAX_POSITION_RATIO: float = 0.8      # æœ€å¤§æŒä»“æ¯”ä¾‹
    
    class Config:
        env_file = ".env"
        case_sensitive = True
        
    @property
    def is_development(self) -> bool:
        return self.ENV == "development"
    
    @property
    def is_production(self) -> bool:
        return self.ENV == "production"
    
    @property
    def is_testing(self) -> bool:
        return self.ENV == "testing"

@lru_cache()
def get_settings() -> Settings:
    """è·å–é…ç½®å•ä¾‹"""
    return Settings()

# å…¨å±€é…ç½®å®ä¾‹
settings = get_settings()
```

## ğŸ—„ï¸ æ•°æ®åº“é…ç½®

### alembic.ini
```ini
# Alembicæ•°æ®åº“è¿ç§»é…ç½®

[alembic]
# è¿ç§»è„šæœ¬ä½ç½®
script_location = app/db/migrations

# æ¨¡æ¿æ–‡ä»¶
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# æ—¶åŒº
timezone = Asia/Shanghai

# æ•°æ®åº“è¿æ¥URL (å°†ä»ç¯å¢ƒå˜é‡è¯»å–)
sqlalchemy.url = 

# æ—¥å¿—é…ç½®
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

### app/db/migrations/env.py
```python
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from app.core.config import settings
from app.db.base import Base

# Alembic Configå¯¹è±¡
config = context.config

# è®¾ç½®æ•°æ®åº“URL
config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)

# é…ç½®æ—¥å¿—
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# ç›®æ ‡å…ƒæ•°æ®
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """ç¦»çº¿è¿ç§»æ¨¡å¼"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online() -> None:
    """åœ¨çº¿è¿ç§»æ¨¡å¼"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

## ğŸ“Š Celeryé…ç½®

### app/tasks/celery_app.py
```python
from celery import Celery
from kombu import Queue
from app.core.config import settings
import os

# åˆ›å»ºCeleryåº”ç”¨
celery_app = Celery(
    "quant-backend",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND,
    include=[
        "app.tasks.backtest_tasks",
        "app.tasks.data_tasks",
        "app.tasks.report_tasks",
        "app.tasks.notification_tasks"
    ]
)

# Celeryé…ç½®
celery_app.conf.update(
    # åºåˆ—åŒ–é…ç½®
    task_serializer=settings.CELERY_TASK_SERIALIZER,
    accept_content=[settings.CELERY_TASK_SERIALIZER],
    result_serializer=settings.CELERY_RESULT_SERIALIZER,
    
    # æ—¶åŒºé…ç½®
    timezone="Asia/Shanghai",
    enable_utc=True,
    
    # ä»»åŠ¡è·¯ç”±é…ç½®
    task_routes={
        "app.tasks.backtest_tasks.*": {"queue": "backtest"},
        "app.tasks.data_tasks.*": {"queue": "data"},
        "app.tasks.report_tasks.*": {"queue": "report"},
        "app.tasks.notification_tasks.*": {"queue": "notification"},
    },
    
    # é˜Ÿåˆ—é…ç½®
    task_queues=(
        Queue("backtest", routing_key="backtest", priority=5),
        Queue("data", routing_key="data", priority=8),
        Queue("report", routing_key="report", priority=3),
        Queue("notification", routing_key="notification", priority=6),
        Queue("default", routing_key="default", priority=1),
    ),
    
    # ä»»åŠ¡æ‰§è¡Œé…ç½®
    task_acks_late=True,
    worker_prefetch_multiplier=1,
    task_reject_on_worker_lost=True,
    task_soft_time_limit=300,  # 5åˆ†é’Ÿè½¯é™åˆ¶
    task_time_limit=600,       # 10åˆ†é’Ÿç¡¬é™åˆ¶
    
    # ç»“æœåç«¯é…ç½®
    result_expires=3600,
    result_persistent=True,
    
    # Workeré…ç½®
    worker_send_task_events=True,
    task_send_sent_event=True,
    worker_hijack_root_logger=False,
    
    # é‡è¯•é…ç½®
    task_default_retry_delay=60,
    task_max_retries=3,
    
    # ç›‘æ§é…ç½®
    worker_enable_remote_control=True,
    
    # å®‰å…¨é…ç½®
    worker_disable_rate_limits=False,
    
    # æ€§èƒ½é…ç½®
    worker_pool_restarts=True,
    worker_max_tasks_per_child=1000,
    worker_log_format="[%(asctime)s: %(levelname)s/%(processName)s] %(message)s",
    worker_task_log_format="[%(asctime)s: %(levelname)s/%(processName)s][%(task_name)s(%(task_id)s)] %(message)s",
)

# ä»»åŠ¡è£…é¥°å™¨é…ç½®
def task_config(**kwargs):
    """ä»»åŠ¡é…ç½®è£…é¥°å™¨"""
    default_config = {
        "bind": True,
        "autoretry_for": (Exception,),
        "retry_kwargs": {"max_retries": 3, "countdown": 60},
        "acks_late": True,
        "reject_on_worker_lost": True,
    }
    default_config.update(kwargs)
    return celery_app.task(**default_config)
```

## ğŸ“ æ—¥å¿—é…ç½®

### app/core/logging.py
```python
import logging
import logging.config
from typing import Dict, Any
import os
from app.core.config import settings

def setup_logging() -> None:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(settings.LOGS_PATH, exist_ok=True)
    
    logging_config: Dict[str, Any] = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "default": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                "datefmt": "%Y-%m-%d %H:%M:%S",
            },
            "detailed": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s",
                "datefmt": "%Y-%m-%d %H:%M:%S",
            },
            "json": {
                "()": "pythonjsonlogger.jsonlogger.JsonFormatter",
                "format": "%(asctime)s %(name)s %(levelname)s %(module)s %(funcName)s %(lineno)d %(message)s",
            },
        },
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
                "level": "DEBUG" if settings.DEBUG else "INFO",
                "formatter": "default",
                "stream": "ext://sys.stdout",
            },
            "file": {
                "class": "logging.handlers.RotatingFileHandler",
                "level": "INFO",
                "formatter": "detailed",
                "filename": f"{settings.LOGS_PATH}/app.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "encoding": "utf8",
            },
            "error_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "level": "ERROR",
                "formatter": "detailed",
                "filename": f"{settings.LOGS_PATH}/error.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "encoding": "utf8",
            },
            "trading_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "level": "INFO",
                "formatter": "json",
                "filename": f"{settings.LOGS_PATH}/trading.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 10,
                "encoding": "utf8",
            },
            "audit_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "level": "INFO",
                "formatter": "json",
                "filename": f"{settings.LOGS_PATH}/audit.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 20,
                "encoding": "utf8",
            },
        },
        "loggers": {
            "": {  # root logger
                "level": settings.LOG_LEVEL,
                "handlers": ["console", "file", "error_file"],
                "propagate": False,
            },
            "app": {
                "level": settings.LOG_LEVEL,
                "handlers": ["console", "file", "error_file"],
                "propagate": False,
            },
            "trading": {
                "level": "INFO",
                "handlers": ["trading_file", "console"],
                "propagate": False,
            },
            "audit": {
                "level": "INFO",
                "handlers": ["audit_file"],
                "propagate": False,
            },
            "uvicorn": {
                "level": "INFO",
                "handlers": ["console"],
                "propagate": False,
            },
            "uvicorn.error": {
                "level": "INFO",
                "handlers": ["console", "error_file"],
                "propagate": False,
            },
            "uvicorn.access": {
                "level": "INFO",
                "handlers": ["console"],
                "propagate": False,
            },
            "sqlalchemy.engine": {
                "level": "WARNING",
                "handlers": ["file"],
                "propagate": False,
            },
            "celery": {
                "level": "INFO",
                "handlers": ["console", "file"],
                "propagate": False,
            },
        },
    }
    
    # ç”Ÿäº§ç¯å¢ƒç‰¹æ®Šé…ç½®
    if settings.is_production:
        # ç”Ÿäº§ç¯å¢ƒä¸è¾“å‡ºåˆ°æ§åˆ¶å°
        logging_config["loggers"][""]["handlers"] = ["file", "error_file"]
        logging_config["loggers"]["app"]["handlers"] = ["file", "error_file"]
        
        # æ·»åŠ Sentryå¤„ç†å™¨
        if settings.SENTRY_DSN:
            logging_config["handlers"]["sentry"] = {
                "class": "sentry_sdk.integrations.logging.SentryHandler",
                "level": "ERROR",
            }
            logging_config["loggers"][""]["handlers"].append("sentry")
            logging_config["loggers"]["app"]["handlers"].append("sentry")
    
    logging.config.dictConfig(logging_config)

# ä¸“ç”¨æ—¥å¿—å™¨
def get_logger(name: str) -> logging.Logger:
    """è·å–æŒ‡å®šåç§°çš„æ—¥å¿—å™¨"""
    return logging.getLogger(name)

# äº¤æ˜“æ—¥å¿—å™¨
trading_logger = get_logger("trading")

# å®¡è®¡æ—¥å¿—å™¨
audit_logger = get_logger("audit")

# åº”ç”¨æ—¥å¿—å™¨
app_logger = get_logger("app")
```

## ğŸ”’ å®‰å…¨é…ç½®

### app/core/security_config.py
```python
from typing import List, Dict, Any
from app.core.config import settings

class SecurityConfig:
    """å®‰å…¨é…ç½®ç±»"""
    
    # CORSé…ç½®
    CORS_CONFIG = {
        "allow_origins": settings.CORS_ORIGINS,
        "allow_credentials": True,
        "allow_methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["*"],
        "expose_headers": ["X-Total-Count", "X-Page-Count"],
    }
    
    # é™æµé…ç½®
    RATE_LIMIT_CONFIG = {
        "default": f"{settings.RATE_LIMIT_REQUESTS}/{settings.RATE_LIMIT_WINDOW}minute",
        "auth": "5/minute",
        "trading": "10/minute",
        "market_data": "100/minute",
    }
    
    # å¯†ç ç­–ç•¥
    PASSWORD_POLICY = {
        "min_length": 8,
        "require_uppercase": True,
        "require_lowercase": True,
        "require_numbers": True,
        "require_special_chars": True,
        "max_age_days": 90,
        "history_count": 5,
    }
    
    # ä¼šè¯é…ç½®
    SESSION_CONFIG = {
        "max_age": settings.JWT_ACCESS_TOKEN_EXPIRE_MINUTES * 60,
        "secure": settings.is_production,
        "httponly": True,
        "samesite": "strict" if settings.is_production else "lax",
    }
    
    # æ–‡ä»¶ä¸Šä¼ å®‰å…¨é…ç½®
    UPLOAD_CONFIG = {
        "max_file_size": settings.MAX_FILE_SIZE,
        "allowed_extensions": [".csv", ".xlsx", ".json", ".txt"],
        "scan_for_malware": settings.is_production,
        "quarantine_suspicious": True,
    }
    
    # APIå®‰å…¨é…ç½®
    API_SECURITY_CONFIG = {
        "require_https": settings.is_production,
        "validate_content_type": True,
        "max_request_size": 10 * 1024 * 1024,  # 10MB
        "timeout": 30,
    }
    
    # æ•°æ®åº“å®‰å…¨é…ç½®
    DATABASE_SECURITY_CONFIG = {
        "use_ssl": settings.is_production,
        "encrypt_sensitive_fields": True,
        "audit_all_changes": True,
        "backup_encryption": True,
    }

# å®‰å…¨ä¸­é—´ä»¶é…ç½®
SECURITY_MIDDLEWARE_CONFIG = {
    "trusted_hosts": settings.ALLOWED_HOSTS,
    "force_https": settings.is_production,
    "hsts_max_age": 31536000 if settings.is_production else 0,
    "hsts_include_subdomains": settings.is_production,
    "referrer_policy": "strict-origin-when-cross-origin",
    "content_type_options": "nosniff",
    "frame_options": "DENY",
    "xss_protection": "1; mode=block",
}
```

## ğŸ³ Dockeré…ç½®

### Dockerfile
```dockerfile
# å¤šé˜¶æ®µæ„å»ºDockerfile
FROM python:3.9-slim as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir --user -r requirements.txt

# ç”Ÿäº§é•œåƒ
FROM python:3.9-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PATH="/root/.local/bin:$PATH"

# åˆ›å»ºåº”ç”¨ç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    libpq5 \
    && rm -rf /var/lib/apt/lists/*

# ä»builderé˜¶æ®µå¤åˆ¶PythonåŒ…
COPY --from=builder /root/.local /root/.local

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p data/logs data/reports data/uploads \
    && chown -R appuser:appuser /app

# åˆ‡æ¢åˆ°åº”ç”¨ç”¨æˆ·
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### docker-compose.yml
```yaml
version: '3.8'

services:
  # åç«¯APIæœåŠ¡
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENV=development
      - DATABASE_URL=postgresql+asyncpg://postgres:password@db:5432/quant_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      - db
      - redis
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    networks:
      - quant-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker
  celery-worker:
    build: .
    command: celery -A app.tasks.celery_app worker --loglevel=info --concurrency=4
    environment:
      - ENV=development
      - DATABASE_URL=postgresql+asyncpg://postgres:password@db:5432/quant_db
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      - db
      - redis
    volumes:
      - ./data:/app/data
    networks:
      - quant-network
    restart: unless-stopped

  # Celery Beat (å®šæ—¶ä»»åŠ¡)
  celery-beat:
    build: .
    command: celery -A app.tasks.celery_app beat --loglevel=info
    environment:
      - ENV=development
      - CELERY_BROKER_URL=redis://redis:6379/1
    depends_on:
      - redis
    volumes:
      - ./data:/app/data
    networks:
      - quant-network
    restart: unless-stopped

  # PostgreSQLæ•°æ®åº“
  db:
    image: timescale/timescaledb:latest-pg14
    environment:
      - POSTGRES_DB=quant_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    networks:
      - quant-network
    restart: unless-stopped

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - quant-network
    restart: unless-stopped
    command: redis-server --appendonly yes

  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - backend
    networks:
      - quant-network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:

networks:
  quant-network:
    driver: bridge
```

